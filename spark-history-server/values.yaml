replicaCount: 1
nameOverride: ""
fullnameOverride: ""

rbac:
  create: true

serviceAccount:
  create: true
  name:

image:
  repository:
  tag: 3.0.1
  pullPolicy: Always

service:
  type: ClusterIP
  port:
   number: 18080
   name: http-historyport
  annotations: {}

environment:
# Note: do not configure Spark history events directory using SPARK_HISTORY_OPTS. It will be
# configured by this chart based on the values in "pvc", "gcs" or "hdfs" attribute.
  # SPARK_HISTORY_OPTS: ...
  # SPARK_DAEMON_MEMORY: 1g
  # SPARK_DAEMON_JAVA_OPTS: ...
  # SPARK_DAEMON_CLASSPATH: ...
  # SPARK_PUBLIC_DNS: ...

podAnnotations:
   cluster-autoscaler.kubernetes.io/safe-to-evict: "false"

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #
  # To let the application start up quickly give it a big limit
  limits:
    cpu: 8000m
    memory: 4Gi
  requests:
    cpu: 8000m
    memory: 4Gi


ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - admin'
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
    nginx.ingress.kubernetes.io/rewrite-target: /
  # kubernetes.io/tls-acme: "true"
  path: /
  hosts: []
  tls: []
  #  - secretName:spark-history-server.example.com
  #    hosts:
  #      - spark-history-server.example.com

pvc:
  # to use a file system path for Spark events dir, set 'enablePVC' to true and mention the
  # name of an already created persistent volume claim in existingClaimName.
  # The volume will be mounted on /data in the pod
  enablePVC: false
  existingClaimName: nfs-pvc
  eventsDir: "/"

# Settings for the sub-chart
# When pvc.enablePVC is true, make sure:
# pvc.existingClaimName == nfs.pvcName
nfs:
  enableExampleNFS: false
  pvName: nfs-pv
  pvcName: nfs-pvc

gcs:
  enableGCS: false
  enableIAM: false
  secret: history-secrets
  key: sparkonk8s.json
  logDirectory: gs://spark-hs/

hdfs:
  hdfsSiteConfigMap: hdfs-site
  coreSiteConfigMap: core-site
  logDirectory: hdfs://hdfs/history/
  HADOOP_CONF_DIR: /etc/hadoop

s3:
  enableS3: true
  enableIAM: true
  # Omit for IAM role-based or provider-based authentication.
  # accessKeyName is an AWS access key ID. Omit for IAM role-based or provider-based authentication.
  # secretKey is AWS secret key. Omit for IAM role-based or provider-based authentication.
  # custom s3 endpoint. Keep default for using aws s3 endpoint
  endpoint: default

wasbs:
  enableWASBS: false
  sasKeyMode: true
  secret: azure-secrets
  sasKeyName: azure-blob-sas-key
  storageAccountKeyName: azure-storage-account-key
  storageAccountNameKeyName: azure-storage-account-name
  containerKeyName: azure-blob-container-name
  logDirectory: wasbs:///spark-hs

imagePullSecrets: []

nodeSelector: {}

tolerations: []

affinity: {}
